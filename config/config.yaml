data:
  data_path: data/filtered
  logs_path: logs

execution:
  n_gpu: 2

general:
  # fps: full parameter sharing;
  # mlp: Multi layer perceptron;
  # stats: to get some statistics about datasets;
  # tsne: to project the data in a 2D space using t-SNE;
  experiment: "mlp"
  # epi: for epigenetic features as input
  # seq: for sequences as input
  # both: to be implemented (for sequence and epigenetic features together as input)
  input_type: "epi"
  n_holdouts: 10
  # ["A549", "GM12878", "HEPG2", "K562"]
  # ["A549", "GM12878", "H1", "HEK293", "HEPG2", "K562" ,"MCF7"]
  cell_lines: ["A549", "GM12878", "H1", "HEK293", "HEPG2", "K562" ,"MCF7"]
  # [{A-E: 1, A-P: 0}, {A-P: 1, I-P: 0}, {A-E: 1, I-E: 0}, {I-E: 1, I-P: 0}, {A-E+A-P: 1, BG: 0}]
  # [{A-E: 1, A-P: 0}, {A-E: 1, I-E: 0}, {I-E: 1, I-P: 0}]
  # [{A-P: 1, I-P: 0}, {A-E+A-P: 1, BG: 0}]
  tasks: [{A-E: 1, A-P: 0}, {A-E: 1, I-E: 0}, {I-E: 1, I-P: 0}]

# Used only in
# CNN Full parameters sharing models
cnn_full_params_sharing:
  epochs: 10
  batch_size: 1000
  #learning_rate: 0.02
  beta_1: 0.9
  beta_2: 0.999
  bayesian_opt:
    max_trials: 30
    num_initial_points: 10
    baseline: 0.2
    patience: 100
    min_delta: 0.005
    hyperparameters:
      kernel_size1: [5, 10]
      kernel_size2: [5, 10]
      units2: [32, 64]
      dense1: [32, 64]
      dense2: [32, 64]
      learning_rate: [0.0, 0.2]

multi_layers_perceptron:
  epochs: 600
  batch_size: 1000
  dropout: 0.2
  decay: 0.01
  momentum: 0.5
  nesterov: True
  bayesian_opt:
    max_trials: 200
    num_initial_points: 30
    baseline: 0.2
    patience: 100
    min_delta: 0.0005
    hyperparameters:
      learning_rate: [0.01, 0.2]
      input_layers: [1, 5]
      input_neurons: [2, 4, 8, 16, 32, 64]
      main_layers: [1, 8]
      main_neurons: [2, 4, 8, 16, 32, 64, 128, 256]
      output_layers: [1, 5]
      output_neurons: [2, 4, 8, 16 , 32, 64]



tsne:
  save_plots: True
  perplexity: 50

